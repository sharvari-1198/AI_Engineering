# -*- coding: utf-8 -*-
"""Trial 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g6aMtE3B8qDbMulMqg5OMy7bxycs6-Yh
"""

!pip install python-docx --quiet

pip install transformers datasets accelerate peft bitsandbytes sentencepiece --quiet

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "gpt2"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

from google.colab import drive
drive.mount('/content/drive')

folder_path = "/content/drive/MyDrive/Deep Learning Lab/Project_Data/Project"

import os
os.listdir("/content/drive/MyDrive/Deep Learning Lab/Project_Data/Project")

import os
from docx import Document

# Path to your folder containing the .docx files
folder_path = "/content/drive/MyDrive/Deep Learning Lab/Project_Data/Project"
poems = []

# Loop through each file and extract text
for filename in sorted(os.listdir(folder_path)):
    if filename.endswith(".docx"):
        doc = Document(os.path.join(folder_path, filename))
        text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
        poems.append(text)

from datasets import Dataset

# Create a dataset dictionary
dataset = Dataset.from_dict({"text": poems})

dataset[0]

import os
import docx
import re

# Function to clean individual poem (remove unnecessary lines like poet's name, sentiment counts, etc.)
def clean_poem_content(poem_text):
    cleaned_lines = []
    for line in poem_text.split("\n"):
        line = line.strip()
        if not line:
            continue
        if ("Poet" in line or
            "Words with respective sentiments" in line or
            re.match(r"Positive: \d+; Negative: \d+; Neutral: \d+", line)):
            continue
        cleaned_lines.append(line)
    return "\n".join(cleaned_lines)

# Function to process all .docx poems and save cleaned versions individually and collectively
def process_and_save_poems(folder_path, output_folder, combined_output_file="cleaned_poems.txt", display=False):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    poems = {}

    with open(os.path.join(output_folder, combined_output_file), 'w', encoding='utf-8') as combined_file:
        for file_name in os.listdir(folder_path):
            if file_name.endswith(".docx"):
                file_path = os.path.join(folder_path, file_name)
                doc = docx.Document(file_path)

                poem_text = "\n".join([para.text for para in doc.paragraphs])
                cleaned_poem = clean_poem_content(poem_text)

                # Save each cleaned poem to an individual .txt file
                individual_path = os.path.join(output_folder, file_name.replace(".docx", ".txt"))
                with open(individual_path, 'w', encoding='utf-8') as f:
                    f.write(cleaned_poem)

                # Also write it into the combined output file
                combined_file.write(f"Poem from {file_name}:\n")
                combined_file.write(cleaned_poem + "\n")
                combined_file.write("="*50 + "\n\n")

                poems[file_name] = cleaned_poem

                if display:
                    print(f"Poem from {file_name}:\n")
                    print(cleaned_poem)
                    print("\n" + "="*50 + "\n")

    print(f"Cleaned poems saved individually to '{output_folder}' and combined in '{combined_output_file}'")
    return poems

# Example usage
folder_path = '/content/drive/MyDrive/Deep Learning Lab/Project_Data/Project'  # Folder with .docx files
output_folder = '/content/drive/MyDrive/Deep Learning Lab/Project_Data/Project/New_Cleaned_Poems/Cleaned_Poems'  # Output folder

# Set display=True if you want to see poems printed on console
poems = process_and_save_poems(folder_path, output_folder, display=False)

print(f"Extracted {len(poems)} poems.")

from transformers import AutoTokenizer
from datasets import Dataset
import os

# Load GPT-2 tokenizer and set pad token
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Folder where your cleaned poems are saved
folder_path = '/content/drive/MyDrive/Deep Learning Lab/Project_Data/Project/New_Cleaned_Poems/Cleaned_Poems'

# Read each poem as a separate text
poem_texts = []
for filename in os.listdir(folder_path):
    if filename.endswith('.txt'):
        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:
            poem_texts.append(file.read())

# Create dataset from list of poems
raw_dataset = Dataset.from_dict({"text": poem_texts})

# Tokenization function (applied to each poem separately)
def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=256)

# Apply tokenization to the dataset
dataset = raw_dataset.map(tokenize, batched=False)

# Check how many samples
print("Number of poems in dataset:", len(dataset))

# Split the dataset into training (70%), validation (15%), and test (15%)
train_data, temp_data = dataset.train_test_split(test_size=0.3).values()  # 70% for training, 30% for validation + test
valid_data, test_data = temp_data.train_test_split(test_size=0.5).values()  # Split the remaining 30% equally for validation and test

# Print the sizes of the datasets
print(f"Training Data: {len(train_data)} poems")
print(f"Validation Data: {len(valid_data)} poems")
print(f"Test Data: {len(test_data)} poems")

!pip install --upgrade transformers --quiet

import transformers
print(transformers.__version__)

print(len(dataset))  # This should give you the total number of samples in your dataset

"""API KEY: 3e48cfea657387bebc69c25e3f5bdacfba85aeb8"""

from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Set the pad token
tokenizer.pad_token = tokenizer.eos_token

# Load the model
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Set up the data collator for language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # GPT-2 is a causal language model, so no masked language modeling
)

# Define the training arguments
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Deep Learning Lab/Output_Trial_1",  # Output directory
    learning_rate=5e-5,  # Learning rate
    per_device_train_batch_size=3,  # Train batch size
    per_device_eval_batch_size=3,  # Evaluation batch size
    weight_decay=0.01,  # Weight decay
    num_train_epochs=3,  # Number of epochs
    logging_dir="/content/drive/MyDrive/Deep Learning Lab/Output_Trial_1/logs",  # Logging directory
    logging_steps=10,  # Log every 10 steps
    save_steps=500,  # Save model every 500 steps
    save_total_limit=2,  # Save only the latest 2 models
)

# Initialize the Trainer
trainer = Trainer(
    model=model,  # The model to train
    args=training_args,  # The training arguments defined earlier
    data_collator=data_collator,  # Data collator
    train_dataset=train_data,  # The training dataset
    eval_dataset=valid_data,  # The validation dataset
)

# Start training
trainer.train()

# Save model and tokenizer manually
model_save_path = "/content/drive/MyDrive/Deep Learning Lab/Output_Trial_1/final-model"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

print(f"Model and tokenizer saved to {model_save_path}")

import torch
import math
from transformers import GPT2LMHeadModel, Trainer

#Load the fine-tuned model (if not already in memory)
model = GPT2LMHeadModel.from_pretrained("/content/drive/MyDrive/Deep Learning Lab/Output_Trial_1/final-model")

# Ensure model is on the right device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Evaluate using the existing Trainer instance
eval_results = trainer.evaluate()

# Print loss and compute perplexity
eval_loss = eval_results["eval_loss"]
eval_perplexity = math.exp(eval_loss)

print(f"üìä Evaluation Loss: {eval_loss:.4f}")
print(f"üìâ Perplexity: {eval_perplexity:.4f}")

# Optional: Log with wandb
import wandb
wandb.log({
    "eval_loss": eval_loss,
    "eval_perplexity": eval_perplexity
})

from transformers import GPT2LMHeadModel, AutoTokenizer
import torch

# Load the fine-tuned model and tokenizer
model_path = "/content/drive/MyDrive/Deep Learning Lab/Output_Trial_1/final-model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# Set padding token (important for GPT-2)
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.eos_token_id

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Function to generate text
def generate_konkani_poem(prompt, max_length=100, num_return_sequences=3):
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

    outputs = model.generate(
        input_ids=input_ids,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        do_sample=True,           # for randomness
        top_k=50,                 # top-k sampling
        top_p=0.95,               # nucleus sampling
        temperature=0.9,          # controls creativity
        pad_token_id=tokenizer.eos_token_id
    )

    generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return generated

# üîç Example Konkani prompt
prompt = "‡§§‡•Å‡§ù‡•á‡§Ç ‡§¶‡§ø‡§≤ ‡§Æ‡•ç‡§π‡§ú‡•á‡§Ç ‡§Ü‡§ï‡§æ‡§∂"  # You can change this

# Generate text
generated_texts = generate_konkani_poem(prompt, max_length=100)

# Print results
for idx, text in enumerate(generated_texts):
    print(f"\nüìù Output {idx+1}:\n{text}")

from transformers import AutoTokenizer, GPT2LMHeadModel
import torch

# Load fine-tuned model and tokenizer
model_path = "/content/drive/MyDrive/Deep Learning Lab/Output_Trial_1/final-model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# Ensure model is in evaluation mode
model.eval()

# Function to generate text with varying temperature
def generate_poem(prompt, temperature):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output = model.generate(
        input_ids,
        max_length=100,
        num_return_sequences=1,
        temperature=temperature,
        top_k=50,
        top_p=0.95,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    generated = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated

# Prompt (in Konkani)
prompt = "‡§Ö‡§ú‡•Ç‡§®‡§ø ‡§Ü‡§Ø‡§≤‡•ã ‡§®‡§æ"  # You can change this to your Konkani test prompt

# Try different temperatures
for temp in [0.7,0.9, 1.0, 1.3]:
    print(f"\n=== Temperature: {temp} ===")
    print(generate_poem(prompt, temperature=temp))



"""loss.backward() ‚Äî computes gradients of the loss w.r.t. each weight.

optimizer.step() ‚Äî updates the weights using the gradients.

optimizer.zero_grad() ‚Äî clears previous gradients so they don‚Äôt accumulate
"""

from transformers import AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch

# Set model to training mode
model.train()

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Create DataLoader from your dataset
train_loader = DataLoader(train_data, batch_size=4, shuffle=True)

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training loop
epochs = 3
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}")

    epoch_loss = 0.0

    for batch in tqdm(train_loader):
        # Move batch to device
        batch = {k: v.to(device) for k, v in batch.items()}

        # Forward pass
        outputs = model(**batch, labels=batch["input_ids"])
        loss = outputs.loss

        # Backward pass
        loss.backward()

        # üöÄ THIS is where weights are updated
        optimizer.step()

        # üßπ Reset gradients
        optimizer.zero_grad()

        # Track loss
        epoch_loss += loss.item()

    avg_loss = epoch_loss / len(train_loader)
    print(f"Average loss for epoch {epoch + 1}: {avg_loss:.4f}")

